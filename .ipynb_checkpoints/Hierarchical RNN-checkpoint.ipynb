{
 "metadata": {
  "name": "",
  "signature": "sha256:786ed3c6fa7299a7ffcf35e01495c2c613bedd0aa1281e5f6f1117e54334998f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import all the modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "import nltk\n",
      "from nltk.corpus import treebank\n",
      "import string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = treebank.words()\n",
      "data = reduce(lambda x, y : x + \" \" + y, data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Construct training and validation set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "valid_size = 1000\n",
      "valid_dataset = data[:valid_size]\n",
      "train_dataset = data[valid_size:]\n",
      "train_size = len(train_dataset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(543269, u'England Journal of Medicine , a forum likely * to bring new atte')\n",
        "(1000, u'Pierre Vinken , 61 years old , will join the board as a nonexecu')\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print valid_dataset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 . Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group . Rudolph Agnew , 55 years old and former chairman of Consolidated Gold Fields PLC , was named *-1 a nonexecutive director of this British industrial conglomerate . A form of asbestos once used * * to make Kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed * to it more than 30 years ago , researchers reported 0 *T*-1 . The asbestos fiber , crocidolite , is unusually resilient once it enters the lungs , with even brief exposures to it causing symptoms that *T*-1 show up decades later , researchers said 0 *T*-2 . Lorillard Inc. , the unit of New York-based Loews Corp. that *T*-2 makes Kent cigarettes , stopped using crocidolite in its Micronite cigarette filters in 1956 . Although preliminary findings were reported *-2 more than a year ago , the latest results appear in today 's New \n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utility functions for conversion between id and characters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary_size = len(set(data)) + 1\n",
      "\n",
      "d = dict()\n",
      "d2 = dict()\n",
      "chars = set(data)\n",
      "for i, c in enumerate(chars):\n",
      "    d[c] = i\n",
      "    d2[i] = c\n",
      "    \n",
      "def char2id(char):\n",
      "    return d[char]\n",
      "  \n",
      "def id2char(i):\n",
      "    return d2[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utility function to create batches of data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "batch_size = 64\n",
      "num_unrollings = 10\n",
      "\n",
      "class BatchGenerator(object):\n",
      "    def __init__(self, text, batch_size, num_unrollings):\n",
      "        self._text = text\n",
      "        self._text_size = len(text)\n",
      "        self._batch_size = batch_size\n",
      "        self._num_unrollings = num_unrollings\n",
      "        segment = self._text_size // batch_size\n",
      "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
      "        self._last_batch = self._next_batch()\n",
      "  \n",
      "    def _next_batch(self):\n",
      "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
      "        for b in range(self._batch_size):\n",
      "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
      "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
      "        return batch\n",
      "  \n",
      "    def next(self):\n",
      "        batches = [self._last_batch]\n",
      "        for step in range(self._num_unrollings):\n",
      "            batches.append(self._next_batch())\n",
      "        self._last_batch = batches[-1]\n",
      "        return batches\n",
      "\n",
      "def characters(probabilities):\n",
      "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
      "\n",
      "def batches2string(batches):\n",
      "    s = [''] * batches[0].shape[0]\n",
      "    for b in batches:\n",
      "        s = [''.join(x) for x in zip(s, characters(b))]\n",
      "    return s\n",
      "\n",
      "train_batches = BatchGenerator(train_dataset, batch_size, num_unrollings)\n",
      "valid_batches = BatchGenerator(valid_dataset, 1, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'England Jou', u' or rock st', u' composite ', u'o occur in ', u'ory capacit', u's *-1 to wo', u' If not for', u' the hero G', u'a Scandinav', u'o get peopl', u'. 31 , 1987', u'bald-faced ', u'0 *T*-1 . B', u' ; the Iowa', u'l below the', u', *-1 to di', u' government', u'k demand fo', u'n such thin', u' Sea Contai', u'on the best', u'y and Mr. B', u'ency was at', u's a greenma', u'e *T*-1 and', u' *T*-1 for ', u'ant , moder', u\"et 's gyrat\", u' . Oliver B', u'ealize that', u'et , N.Y. ,', u'% of the si', u'rom $ 75 mi', u'e opened in', u'ted to the ', u' listeners ', u'ed profitab', u'tes include', u' discussing', u'flirted wit', u'ust pay for', u\"ard 's most\", u'orp . `` Do', u'occurs not ', u'wyers repor', u\" 'd much ra\", u'e Inc. and ', u'ontributed ', u'ced the For', u'pects *-2 t', u'a Legg Maso', u'was that no', u'vernment so', u'airman Step', u'sing the mo', u'ry , said 0', u'*-3 offset ', u'ed with $ 6', u'ast Berlin ', u'hich *T*-1 ', u'as received', u'l campaign ', u'es , it add', u'issues , su']\n",
        "[u'urnal of Me', u'tars than f', u' trading . ', u' about two ', u'ty 500 time', u'ork out a p', u'r a 59.6 % ', u\"God 's phon\", u'vian price ', u\"le 's atten\", u'7 . Japanes', u' nature of ', u'But others ', u'a Test of B', u\"e year 's d\", u'iscuss the ', u't leaders r', u'ollowing a ', u'ngs as what', u\"iners ' pla\", u't wine 0 St', u'Black say 0', u't 1.8500 ma', u'ailer tryin', u'd the tradi', u' $ 15,000 *', u'rn note in ', u'tions can t', u'Berliner Be', u't the most ', u', $ 30,000 ', u'ize of the ', u'illion *U* ', u'n September', u' recent inc', u' around the', u'bility and ', u'e Susan Pet', u'g arms cont', u'th a conver', u'r his own t', u't respected', u'o you make ', u' because of', u'rted that p', u'ather see t', u' Standard &', u' # 34 milli', u'rd Thunderb', u'to post sal', u'on Wood Wal', u'on-farm pay', u'ources and ', u'phen Wolf r', u'ore-advance', u'0 *T*-2 Fre', u' the impact', u'6.4 million', u' government', u' boosts the', u'd *T*-1 . B', u' . The comp', u'ded 0 *T*-1', u'uch as a ri']\n",
        "[u'Pi']\n",
        "[u'ie']\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utility function to find probabilities"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logprob(predictions, labels):\n",
      "    predictions[predictions < 1e-10] = 1e-10\n",
      "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
      "\n",
      "def sample_distribution(distribution):\n",
      "    r = random.uniform(0, 1)\n",
      "    s = 0\n",
      "    for i in range(len(distribution)):\n",
      "        s += distribution[i]\n",
      "        if s >= r:\n",
      "            return i\n",
      "    return len(distribution) - 1\n",
      "\n",
      "def sample(prediction):\n",
      "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
      "    p[0, sample_distribution(prediction[0])] = 1.0\n",
      "    return p\n",
      "\n",
      "def random_distribution():\n",
      "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
      "    return b/np.sum(b, 1)[:,None]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Graph of tensorflow for lstm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_nodes = 64\n",
      "\n",
      "graph = tf.Graph()\n",
      "\n",
      "with graph.as_default():\n",
      "    \n",
      "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "    \n",
      "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
      "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
      "\n",
      "    def lstm_cell(i, o, state):\n",
      "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
      "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
      "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
      "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
      "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
      "        return output_gate * tf.tanh(state), state\n",
      "\n",
      "    train_dataset = list()\n",
      "    for _ in range(num_unrollings + 1):\n",
      "        train_dataset.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
      "    train_inputs = train_dataset[:num_unrollings]\n",
      "    train_labels = train_dataset[1:]\n",
      "\n",
      "    outputs = list()\n",
      "    output = saved_output\n",
      "    state = saved_state\n",
      "    for i in train_inputs:\n",
      "        output, state = lstm_cell(i, output, state)\n",
      "        outputs.append(output)\n",
      "\n",
      "    with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
      "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
      "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
      "\n",
      "    global_step = tf.Variable(0)\n",
      "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
      "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
      "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
      "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
      "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
      "\n",
      "    train_prediction = tf.nn.softmax(logits)\n",
      "  \n",
      "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
      "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
      "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
      "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
      "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
      "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 7001\n",
      "summary_frequency = 100\n",
      "\n",
      "with tf.Session(graph = graph) as session:\n",
      "    \n",
      "    tf.initialize_all_variables().run()\n",
      "    print('Initialized')\n",
      "    \n",
      "    mean_loss = 0\n",
      "    \n",
      "    for step in range(num_steps):\n",
      "        \n",
      "        batches = train_batches.next()\n",
      "        feed_dict = dict()\n",
      "        \n",
      "        for i in range(num_unrollings + 1):\n",
      "            \n",
      "            feed_dict[train_dataset[i]] = batches[i]\n",
      "        \n",
      "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
      "        \n",
      "        mean_loss += l\n",
      "        if step % summary_frequency == 0:\n",
      "            if step > 0:\n",
      "                mean_loss = mean_loss / summary_frequency\n",
      "\n",
      "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
      "            mean_loss = 0\n",
      "            labels = np.concatenate(list(batches)[1:])\n",
      "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
      "            \n",
      "            if step % (summary_frequency * 10) == 0:\n",
      "                \n",
      "                print('=' * 80)\n",
      "                \n",
      "                for _ in range(5):\n",
      "                    \n",
      "                    feed = sample(random_distribution())\n",
      "                    sentence = characters(feed)[0]\n",
      "                    reset_sample_state.run()\n",
      "                    \n",
      "                    for _ in range(79):\n",
      "                        prediction = sample_prediction.eval({sample_input: feed})\n",
      "                        feed = sample(prediction)\n",
      "                        sentence += characters(feed)[0]\n",
      "                    \n",
      "                    print(sentence)\n",
      "                \n",
      "                print('=' * 80)\n",
      "            \n",
      "            reset_sample_state.run()\n",
      "            valid_logprob = 0\n",
      "            for _ in range(valid_size):\n",
      "                b = valid_batches.next()\n",
      "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
      "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
      "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "Cannot interpret feed_dict key as Tensor: The name 'a' looks like an (invalid) Operation name, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\".",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-26-8f4086b2c949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmean_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    870\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[0;32m--> 872\u001b[0;31m                             + e.args[0])\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: The name 'a' looks like an (invalid) Operation name, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\"."
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}