{
 "metadata": {
  "name": "",
  "signature": "sha256:e14b91a9703517161d21b65a52017e2608a8a6789fe1b328fbb02a423fc2b77a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import all the modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "import nltk\n",
      "from nltk.corpus import treebank\n",
      "import string\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = treebank.words()\n",
      "data = reduce(lambda x, y : x + \" \" + y, data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Construct training and validation set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "valid_size = 1000\n",
      "valid_dataset = data[:valid_size]\n",
      "train_dataset = data[valid_size:]\n",
      "train_size = len(train_dataset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print valid_dataset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 . Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group . Rudolph Agnew , 55 years old and former chairman of Consolidated Gold Fields PLC , was named *-1 a nonexecutive director of this British industrial conglomerate . A form of asbestos once used * * to make Kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed * to it more than 30 years ago , researchers reported 0 *T*-1 . The asbestos fiber , crocidolite , is unusually resilient once it enters the lungs , with even brief exposures to it causing symptoms that *T*-1 show up decades later , researchers said 0 *T*-2 . Lorillard Inc. , the unit of New York-based Loews Corp. that *T*-2 makes Kent cigarettes , stopped using crocidolite in its Micronite cigarette filters in 1956 . Although preliminary findings were reported *-2 more than a year ago , the latest results appear in today 's New \n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utility functions for conversion between id and characters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary_size = len(set(data))\n",
      "\n",
      "d = dict()\n",
      "d2 = dict()\n",
      "chars = set(data)\n",
      "for i, c in enumerate(chars):\n",
      "    d[c] = i\n",
      "    d2[i] = c\n",
      "    \n",
      "def char2id(char):\n",
      "    return d[char]\n",
      "  \n",
      "def id2char(i):\n",
      "    return d2[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utility function to create batches of data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "batch_size = 64\n",
      "num_unrollings = 10\n",
      "\n",
      "class BatchGenerator(object):\n",
      "    def __init__(self, text, batch_size, num_unrollings):\n",
      "        self._text = text\n",
      "        self._text_size = len(text)\n",
      "        self._batch_size = batch_size\n",
      "        self._num_unrollings = num_unrollings\n",
      "        segment = self._text_size // batch_size\n",
      "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
      "        self._last_batch = self._next_batch()\n",
      "  \n",
      "    def _next_batch(self):\n",
      "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
      "        for b in range(self._batch_size):\n",
      "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
      "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
      "        return batch\n",
      "  \n",
      "    def next(self):\n",
      "        batches = [self._last_batch]\n",
      "        for step in range(self._num_unrollings):\n",
      "            batches.append(self._next_batch())\n",
      "        self._last_batch = batches[-1]\n",
      "        return batches\n",
      "\n",
      "def characters(probabilities):\n",
      "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
      "\n",
      "def batches2string(batches):\n",
      "    s = [''] * batches[0].shape[0]\n",
      "    for b in batches:\n",
      "        s = [''.join(x) for x in zip(s, characters(b))]\n",
      "    return s\n",
      "\n",
      "train_batches = BatchGenerator(train_dataset, batch_size, num_unrollings)\n",
      "valid_batches = BatchGenerator(valid_dataset, 1, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utility function to find probabilities"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logprob(predictions, labels):\n",
      "    predictions[predictions < 1e-10] = 1e-10\n",
      "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
      "\n",
      "def sample_distribution(distribution):\n",
      "    r = random.uniform(0, 1)\n",
      "    s = 0\n",
      "    for i in range(len(distribution)):\n",
      "        s += distribution[i]\n",
      "        if s >= r:\n",
      "            return i\n",
      "    return len(distribution) - 1\n",
      "\n",
      "def sample(prediction):\n",
      "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
      "    p[0, sample_distribution(prediction[0])] = 1.0\n",
      "    return p\n",
      "\n",
      "def random_distribution():\n",
      "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
      "    return b/np.sum(b, 1)[:,None]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Graph of tensorflow for lstm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_nodes = 64\n",
      "\n",
      "graph = tf.Graph()\n",
      "\n",
      "with graph.as_default():\n",
      "    \n",
      "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "    \n",
      "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
      "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
      "\n",
      "    def lstm_cell(i, o, state):\n",
      "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
      "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
      "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
      "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
      "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
      "        return output_gate * tf.tanh(state), state\n",
      "\n",
      "    train_dataset = list()\n",
      "    for _ in range(num_unrollings + 1):\n",
      "        train_dataset.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
      "    train_inputs = train_dataset[:num_unrollings]\n",
      "    train_labels = train_dataset[1:]\n",
      "\n",
      "    outputs = list()\n",
      "    output = saved_output\n",
      "    state = saved_state\n",
      "    for i in train_inputs:\n",
      "        output, state = lstm_cell(i, output, state)\n",
      "        outputs.append(output)\n",
      "\n",
      "    with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
      "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
      "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
      "\n",
      "    global_step = tf.Variable(0)\n",
      "    learning_rate = tf.train.exponential_decay(10.0, global_step, 20000, 0.1, staircase=True)\n",
      "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
      "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
      "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
      "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
      "\n",
      "    train_prediction = tf.nn.softmax(logits)\n",
      "  \n",
      "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
      "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
      "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
      "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
      "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
      "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 100001\n",
      "summary_frequency = 10000\n",
      "\n",
      "with tf.Session(graph = graph) as session:\n",
      "    \n",
      "    tf.initialize_all_variables().run()\n",
      "    print('Initialized')\n",
      "    \n",
      "    mean_loss = 0\n",
      "    \n",
      "    for step in range(num_steps):\n",
      "        \n",
      "        batches = train_batches.next()\n",
      "        feed_dict = dict()\n",
      "        \n",
      "        for i in range(num_unrollings + 1):\n",
      "            feed_dict[train_dataset[i]] = batches[i]\n",
      "        \n",
      "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
      "        \n",
      "        mean_loss += l\n",
      "        if step % summary_frequency == 0:\n",
      "            if step > 0:\n",
      "                mean_loss = mean_loss / summary_frequency\n",
      "\n",
      "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
      "            mean_loss = 0\n",
      "            labels = np.concatenate(list(batches)[1:])\n",
      "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
      "            \n",
      "            if step % (summary_frequency * 2) == 0:\n",
      "                \n",
      "                print('=' * 80)\n",
      "                \n",
      "                for _ in range(5):\n",
      "                    \n",
      "                    feed = sample(random_distribution())\n",
      "                    sentence = characters(feed)[0]\n",
      "                    reset_sample_state.run()\n",
      "                    \n",
      "                    for _ in range(79):\n",
      "                        prediction = sample_prediction.eval({sample_input: feed})\n",
      "                        feed = sample(prediction)\n",
      "                        sentence += characters(feed)[0]\n",
      "                    \n",
      "                    print(sentence)\n",
      "                \n",
      "                print('=' * 80)\n",
      "            \n",
      "            reset_sample_state.run()\n",
      "            valid_logprob = 0\n",
      "            for _ in range(valid_size):\n",
      "                b = valid_batches.next()\n",
      "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
      "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
      "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
      "            \n",
      "    saver = tf.train.Saver()\n",
      "    saver.save(session, \"./Model/lstm.ckpt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "Average loss at step 0: 4.381774 learning rate: 10.000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 79.98\n",
        "================================================================================\n",
        "D; #udlP Lj%?rN71e&e97,cas o`AZoCNzMlnUZ;iO8M%rJ 4GhAQbJowu%f3MF3j# qr2W! OLgskW"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "P.TQQ5u S6\\f-v\\stA@a9fJet`1, sl,m *tFC. 4KZ0a?t.fpooK9;bm%cO:D cH, Bef6Fx7Wo43o,\n",
        "kKWnEvy'i'hY2zRbv0eJM yHWgPY6Fa*5KKd.`VtKg#AYQ@*& #O pE1Rb5,B-o--/%'\\fbr6.`/NL:t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "LT%0c13V/i 9cQ7d4ku c$ssZh \\lcw Ixqzh.r bUVYF;GvF  j,y4:Zi9Unhq #ajxtmwTLamu3 6I\n",
        "9`uH6:f'i3t t?XRxu FdX?ie3f$!HrVRx  jNLWlfJ?&2A$#e,vm as*a9x,ZW2IiCderN 8kt\\n@ t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================================================================\n",
        "Validation set perplexity: 45.43"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 10000: 1.620522 learning rate: 10.000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 4.69\n",
        "Validation set perplexity: 5.56"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 20000: 1.457377 learning rate: 1.000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 4.04\n",
        "================================================================================\n",
        "manned due anyther Surccirper , the big placed such assulita booled arguing chue\n",
        "mee , bulriater , *-1 said 0 weeks of Frank People state anything purposed this "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "D have a cempia duerging are underlyonly would largest of test Organ shares for \n",
        "Y plareol 34 fillis annual each students vehicles . New Co . Gring 's crisiatiiz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "part an index , assect Seitin's bankers from price *T*-2 Mas Andde . Chere good-\n",
        "================================================================================\n",
        "Validation set perplexity: 5.39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 30000: 1.362390 learning rate: 1.000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 3.90\n",
        "Validation set perplexity: 5.03"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 40000: 1.347345 learning rate: 0.100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 4.21\n",
        "================================================================================\n",
        ", twon exchangan , below that it *EXP*-1 authorit with nations , which *T*-162 o\n",
        "Mrized Declined Stankon Inc. effecting unpays , '' says *T*-1 's heari Gaining C\n",
        "2 stard preverent have cornigations ' Sept. 51 and a secured `` service defendin"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Ney hay drugting his all mine vary of the Norually agencablespol , they completi\n",
        "comparal has disobirity boost and a by the likely investor a boventers , a could"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================================================================\n",
        "Validation set perplexity: 5.09"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 50000: 1.334895 learning rate: 0.100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 3.80\n",
        "Validation set perplexity: 5.03"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 60000: 1.333231 learning rate: 0.010000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 3.92\n",
        "================================================================================\n",
        "Heforse Computer lugol makers can open end Tuesday Exchang Incour-? Alle defense\n",
        " for a outra in federal point * to has member World has IT*-13 plamicome budget "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6 is composite defects : 14 reffertors *-3 *T*-1 signining tegms ipage , Octecod\n",
        "ing the file , wlo numuriss workers . Ratgeline Inc0 in Jut again *-1 to gained "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Z of upstan Industry nekup so enough safety cases affualiaty Ackin debts , it ma\n",
        "================================================================================\n",
        "Validation set perplexity: 5.02"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 70000: 1.331130 learning rate: 0.010000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 3.70\n",
        "Validation set perplexity: 5.02"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 80000: 1.330884 learning rate: 0.001000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 3.51\n",
        "================================================================================\n",
        "I , the United Pre making chiritional Call wo n't denies about Justice D.D. yesl\n",
        "zitent of the nonetiveing plangements operatives of the new suscallich close his"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/itted it is problem short of jeditorized if the reform the Corp. , *-7-1983-amp\n",
        "compasiunts * to the she office . Treatury , if officials Bro conwimutic is prov"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/istor note a consence blurk brokal futures stock who *T*-201 is n't car or adva\n",
        "================================================================================\n",
        "Validation set perplexity: 5.02"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 90000: 1.330591 learning rate: 0.001000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 4.33\n",
        "Validation set perplexity: 5.02"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 100000: 1.331157 learning rate: 0.000100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 3.93\n",
        "================================================================================\n",
        "0 Battorially , wheraornicatics finance of the depositsen . Those much was offic\n",
        "Geging *-1 for alcut for a better imports prices topare that a computer not pour"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        ". Ricke-Staninsy cringed *-1 authority , and 367-8.8 million *U* dives wines . T\n",
        "uting wheres *T*-1 . Madies 's 21 % and `` seen that it 's limutut . A news the "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "meanile of uslyster mortgage teasy in U.S. an 90 distaigratials bookred at * are\n",
        "================================================================================\n",
        "Validation set perplexity: 5.02"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}