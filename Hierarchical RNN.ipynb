{
 "metadata": {
  "name": "",
  "signature": "sha256:eb5c6a724ec5c800da0cb41f7106ecfc7df22ff55cff50ae1f4a362b0f39fa1c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import all the modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "import nltk\n",
      "from nltk.corpus import treebank\n",
      "import string\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = treebank.words()\n",
      "data = reduce(lambda x, y : x + \" \" + y, data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Construct training and validation set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "valid_size = 1000\n",
      "valid_dataset = data[:valid_size]\n",
      "train_dataset = data[valid_size:]\n",
      "train_size = len(train_dataset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print valid_dataset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 . Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group . Rudolph Agnew , 55 years old and former chairman of Consolidated Gold Fields PLC , was named *-1 a nonexecutive director of this British industrial conglomerate . A form of asbestos once used * * to make Kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed * to it more than 30 years ago , researchers reported 0 *T*-1 . The asbestos fiber , crocidolite , is unusually resilient once it enters the lungs , with even brief exposures to it causing symptoms that *T*-1 show up decades later , researchers said 0 *T*-2 . Lorillard Inc. , the unit of New York-based Loews Corp. that *T*-2 makes Kent cigarettes , stopped using crocidolite in its Micronite cigarette filters in 1956 . Although preliminary findings were reported *-2 more than a year ago , the latest results appear in today 's New \n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utility functions for conversion between id and characters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary_size = len(set(data))\n",
      "\n",
      "d = dict()\n",
      "d2 = dict()\n",
      "chars = set(data)\n",
      "for i, c in enumerate(chars):\n",
      "    d[c] = i\n",
      "    d2[i] = c\n",
      "    \n",
      "def char2id(char):\n",
      "    return d[char]\n",
      "  \n",
      "def id2char(i):\n",
      "    return d2[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utility function to create batches of data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "batch_size = 64\n",
      "num_unrollings = 10\n",
      "\n",
      "class BatchGenerator(object):\n",
      "    def __init__(self, text, batch_size, num_unrollings):\n",
      "        self._text = text\n",
      "        self._text_size = len(text)\n",
      "        self._batch_size = batch_size\n",
      "        self._num_unrollings = num_unrollings\n",
      "        segment = self._text_size // batch_size\n",
      "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
      "        self._last_batch = self._next_batch()\n",
      "  \n",
      "    def _next_batch(self):\n",
      "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
      "        for b in range(self._batch_size):\n",
      "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
      "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
      "        return batch\n",
      "  \n",
      "    def next(self):\n",
      "        batches = [self._last_batch]\n",
      "        for step in range(self._num_unrollings):\n",
      "            batches.append(self._next_batch())\n",
      "        self._last_batch = batches[-1]\n",
      "        return batches\n",
      "\n",
      "def characters(probabilities):\n",
      "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
      "\n",
      "def batches2string(batches):\n",
      "    s = [''] * batches[0].shape[0]\n",
      "    for b in batches:\n",
      "        s = [''.join(x) for x in zip(s, characters(b))]\n",
      "    return s\n",
      "\n",
      "train_batches = BatchGenerator(train_dataset, batch_size, num_unrollings)\n",
      "valid_batches = BatchGenerator(valid_dataset, 1, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utility function to find probabilities"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logprob(predictions, labels):\n",
      "    predictions[predictions < 1e-10] = 1e-10\n",
      "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
      "\n",
      "def sample_distribution(distribution):\n",
      "    r = random.uniform(0, 1)\n",
      "    s = 0\n",
      "    for i in range(len(distribution)):\n",
      "        s += distribution[i]\n",
      "        if s >= r:\n",
      "            return i\n",
      "    return len(distribution) - 1\n",
      "\n",
      "def sample(prediction):\n",
      "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
      "    p[0, sample_distribution(prediction[0])] = 1.0\n",
      "    return p\n",
      "\n",
      "def random_distribution():\n",
      "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
      "    return b/np.sum(b, 1)[:,None]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Graph of tensorflow for lstm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_nodes = 64\n",
      "\n",
      "graph = tf.Graph()\n",
      "\n",
      "with graph.as_default():\n",
      "    \n",
      "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
      "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    \n",
      "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "    \n",
      "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
      "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
      "\n",
      "    def lstm_cell(i, o, state):\n",
      "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
      "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
      "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
      "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
      "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
      "        return output_gate * tf.tanh(state), state\n",
      "\n",
      "    train_dataset = list()\n",
      "    for _ in range(num_unrollings + 1):\n",
      "        train_dataset.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
      "    train_inputs = train_dataset[:num_unrollings]\n",
      "    train_labels = train_dataset[1:]\n",
      "\n",
      "    outputs = list()\n",
      "    output = saved_output\n",
      "    state = saved_state\n",
      "    for i in train_inputs:\n",
      "        output, state = lstm_cell(i, output, state)\n",
      "        outputs.append(output)\n",
      "\n",
      "    with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
      "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
      "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
      "\n",
      "    global_step = tf.Variable(0)\n",
      "    learning_rate = tf.train.exponential_decay(10.0, global_step, 20000, 0.1, staircase=True)\n",
      "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
      "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
      "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
      "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
      "\n",
      "    train_prediction = tf.nn.softmax(logits)\n",
      "  \n",
      "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
      "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
      "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
      "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
      "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
      "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 100001\n",
      "summary_frequency = 10000\n",
      "\n",
      "with tf.Session(graph = graph) as session:\n",
      "    \n",
      "    tf.initialize_all_variables().run()\n",
      "    print('Initialized')\n",
      "    \n",
      "    mean_loss = 0\n",
      "    \n",
      "    for step in range(num_steps):\n",
      "        \n",
      "        batches = train_batches.next()\n",
      "        feed_dict = dict()\n",
      "        \n",
      "        for i in range(num_unrollings + 1):\n",
      "            feed_dict[train_dataset[i]] = batches[i]\n",
      "        \n",
      "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
      "        \n",
      "        mean_loss += l\n",
      "        if step % summary_frequency == 0:\n",
      "            if step > 0:\n",
      "                mean_loss = mean_loss / summary_frequency\n",
      "\n",
      "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
      "            mean_loss = 0\n",
      "            labels = np.concatenate(list(batches)[1:])\n",
      "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
      "            \n",
      "            if step % (summary_frequency * 2) == 0:\n",
      "                \n",
      "                print('=' * 80)\n",
      "                \n",
      "                for _ in range(5):\n",
      "                    \n",
      "                    feed = sample(random_distribution())\n",
      "                    sentence = characters(feed)[0]\n",
      "                    reset_sample_state.run()\n",
      "                    \n",
      "                    for _ in range(79):\n",
      "                        prediction = sample_prediction.eval({sample_input: feed})\n",
      "                        feed = sample(prediction)\n",
      "                        sentence += characters(feed)[0]\n",
      "                    \n",
      "                    print(sentence)\n",
      "                \n",
      "                print('=' * 80)\n",
      "            \n",
      "            reset_sample_state.run()\n",
      "            valid_logprob = 0\n",
      "            for _ in range(valid_size):\n",
      "                b = valid_batches.next()\n",
      "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
      "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
      "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "Average loss at step 0: 4.384221 learning rate: 10.000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 80.18\n",
        "================================================================================\n",
        "W5hx 8CN3M0iLN0` ;T;i4 aG sHsbjH2woFU vUKp E UTU%pIMR9N;KbMnr%4NCa5Bq -VNk7a4 nO"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "i1UL#C%3Ym;f#IVM/gW*9T6PIrH0c@Bz/7?9EfjGHl/UaoBbf.3uO11 'Myn?\\etD9SXaosiWh4LMOQU\n",
        "l&21tD,hJ*TtC\\ nY8zjw%@E1O*cTR9jb4 1-z;, XX*MIDS v5Zwu6b p;S2Y&#sE&l`aR ?$Fe:g4n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "rtCkJ 0cXt0`cae R8e-7JyajitXn,?5l!wfGodm u5%gjHSl.Ie\\TGl\\z5l9i8m!nNld;H PIoGAxxJ\n",
        "R  3S$ eBgi`'F#t1f/0HjoL@loiowT5!5z4v%q%Rbti. fUT' tD 7Nkoq ;ah# Q,RrR XB-,mPCsO"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================================================================\n",
        "Validation set perplexity: 46.96"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 10000: 1.624759 learning rate: 10.000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 4.80\n",
        "Validation set perplexity: 5.64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 20000: 1.462028 learning rate: 1.000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 4.10\n",
        "================================================================================\n",
        "des , which *T*-1 may be arso managed buy gervalia fatheres goeder than dantaryt\n",
        "% to rise insist , regard champent volation . Tellowizbrruge seat than chiefferd"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X the concern about 1,540,040 mirlite a treatipatement . ON memersion cturders t\n",
        "latemally stares rathes President stall months harke two suse main-legicled foud"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Viled Lybogh John Alynly , * for Univelized things the fire also of the feast to\n",
        "================================================================================\n",
        "Validation set perplexity: 5.47"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 30000: 1.365572 learning rate: 1.000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 4.03\n",
        "Validation set perplexity: 4.98"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 40000: 1.350791 learning rate: 0.100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 3.66\n",
        "================================================================================\n",
        "8 espect * to meaty *-1 to duties comloy for and Columbia Farl gains that *T*-1 \n",
        "T develuyed daking ways partly day that be does an diviliod opportuuse , den Sep"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vairs , how in who *T*-1 whriblelized up the required *-46 , $ 10 billion *U* an\n",
        "E all currently resuing wholiegs , such financials , a lower own boights 0 it sa"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 's serious . A service manager Ston John Sale scient Cara Steem , is pursue man\n",
        "================================================================================\n",
        "Validation set perplexity: 4.94"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 50000: 1.337451 learning rate: 0.100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 3.76\n",
        "Validation set perplexity: 4.93"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 60000: 1.335283 learning rate: 0.010000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 4.11\n",
        "================================================================================\n",
        "4 *-4 to earning manufactor government-outlessions from Indatence of the members\n",
        "4 also provides that `` traders indicator *?* , conflicive test . It longs shoul"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0 muy at the New York --RRB\\ . Fromiciferded if the Dingivized Kndals -LRB- I vo\n",
        "ations and traders on *T*-2 ; OCHA : 8 23.6 million ; $ 50-boskides For any decl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/one and collevings could yield , those Indurrally intakes in the referring viol\n",
        "================================================================================\n",
        "Validation set perplexity: 4.92"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 70000: 1.333626 learning rate: 0.010000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 3.63\n",
        "Validation set perplexity: 4.93"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 80000: 1.333878 learning rate: 0.001000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 3.36\n",
        "================================================================================\n",
        "y of the cost becomes travemordest as imports . In Whise on the flatipal relativ\n",
        "Japy , 17 choultar IIH. Jown , as only umsties law . Indostemerlin group to the "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "; cality at Pee carry 50 % to bearding at are lose U.S. and appoonged pounders a\n",
        "S . Newsweek Gentman , News 0 * to hmountes of shares . The U.S. returning the l"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ths of difmissions paper maverngzorawning mearoun in the varyets off president ,\n",
        "================================================================================\n",
        "Validation set perplexity: 4.93"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 90000: 1.333766 learning rate: 0.001000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 4.47\n",
        "Validation set perplexity: 4.93"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Average loss at step 100000: 1.333399 learning rate: 0.000100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch perplexity: 4.06\n",
        "================================================================================\n",
        "4 49-concen officials onlyssiten with Meors or importures for 92 just growth has\n",
        "4 Mr. Grain Inc. . He controlulemal pincusto Leach , said *T*-1 . Ricimn Mr. Bau"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "H as official of the assets of the monthle leveraweling credit , invesuest that \n",
        "' of program trades secrecing newspaper , Rurk . Mantan Congress cost , emethano"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "K or $ 55.1 million *U* a prinses -RCB-3 index funds in other nation chose , '' \n",
        "================================================================================\n",
        "Validation set perplexity: 4.93"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}